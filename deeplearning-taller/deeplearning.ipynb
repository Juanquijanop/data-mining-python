{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0c82308-7694-4df9-ae44-771d7974a461",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b65e8d02-c1ba-436c-b789-7cb9692dfe71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los datos\n",
    "data = pd.read_csv('LACrimesData.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b926ea1-5177-415f-803d-4500bee4becb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas con valores NaN: ['Date Rptd', 'DATE OCC', 'TIME OCC', 'AREA', 'AREA NAME', 'Rpt Dist No', 'Part 1-2', 'Crm Cd', 'Crm Cd Desc', 'Mocodes', 'Vict Age', 'Vict Sex', 'Vict Descent', 'Premis Cd', 'Premis Desc', 'Weapon Used Cd', 'Weapon Desc', 'Status', 'Status Desc', 'Crm Cd 1', 'Crm Cd 2', 'Crm Cd 3', 'Crm Cd 4', 'LOCATION', 'Cross Street', 'LAT', 'LON']\n",
      "Columnas con valores infinitos: []\n"
     ]
    }
   ],
   "source": [
    "# Detectar columnas con valores NaN\n",
    "cols_with_nan = data.columns[data.isna().any()].tolist()\n",
    "\n",
    "# Seleccionar solo columnas numéricas para detectar valores infinitos\n",
    "numeric_data = data.select_dtypes(include=[np.number])\n",
    "cols_with_inf = numeric_data.columns[np.isinf(numeric_data).any()].tolist()\n",
    "\n",
    "print(\"Columnas con valores NaN:\", cols_with_nan)\n",
    "print(\"Columnas con valores infinitos:\", cols_with_inf)\n",
    "\n",
    "# Columnas esenciales para el análisis\n",
    "required_columns = ['DATE OCC', 'HOUR', 'DAY_OF_WEEK', 'MONTH', 'Crm Cd Desc']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6c33d66f-6927-4bff-8d18-494e282c67ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/utils/extmath.py:1050: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/utils/extmath.py:1055: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/utils/extmath.py:1075: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/opt/conda/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1ms/step - accuracy: 0.0032 - loss: nan - val_accuracy: 0.0034 - val_loss: nan\n",
      "Epoch 2/20\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.0032 - loss: nan - val_accuracy: 0.0034 - val_loss: nan\n",
      "Epoch 3/20\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.0031 - loss: nan - val_accuracy: 0.0034 - val_loss: nan\n",
      "Epoch 4/20\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.0031 - loss: nan - val_accuracy: 0.0034 - val_loss: nan\n",
      "Epoch 5/20\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.0032 - loss: nan - val_accuracy: 0.0034 - val_loss: nan\n",
      "Epoch 6/20\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.0030 - loss: nan - val_accuracy: 0.0034 - val_loss: nan\n",
      "Epoch 7/20\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.0030 - loss: nan - val_accuracy: 0.0034 - val_loss: nan\n",
      "Epoch 8/20\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.0031 - loss: nan - val_accuracy: 0.0034 - val_loss: nan\n",
      "Epoch 9/20\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.0032 - loss: nan - val_accuracy: 0.0034 - val_loss: nan\n",
      "Epoch 10/20\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.0032 - loss: nan - val_accuracy: 0.0034 - val_loss: nan\n",
      "Epoch 11/20\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.0031 - loss: nan - val_accuracy: 0.0034 - val_loss: nan\n",
      "Epoch 12/20\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.0032 - loss: nan - val_accuracy: 0.0034 - val_loss: nan\n",
      "Epoch 13/20\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.0031 - loss: nan - val_accuracy: 0.0034 - val_loss: nan\n",
      "Epoch 14/20\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.0029 - loss: nan - val_accuracy: 0.0034 - val_loss: nan\n",
      "Epoch 15/20\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.0032 - loss: nan - val_accuracy: 0.0034 - val_loss: nan\n",
      "Epoch 16/20\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.0032 - loss: nan - val_accuracy: 0.0034 - val_loss: nan\n",
      "Epoch 17/20\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.0031 - loss: nan - val_accuracy: 0.0034 - val_loss: nan\n",
      "Epoch 18/20\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.0033 - loss: nan - val_accuracy: 0.0034 - val_loss: nan\n",
      "Epoch 19/20\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.0031 - loss: nan - val_accuracy: 0.0034 - val_loss: nan\n",
      "Epoch 20/20\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.0032 - loss: nan - val_accuracy: 0.0034 - val_loss: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0xfffeee1c6290>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocesamiento de datos\n",
    "data['DATE OCC'] = pd.to_datetime(data['DATE OCC'], format='%m/%d/%Y %H:%M:%S', errors='coerce')\n",
    "data['HOUR'] = data['DATE OCC'].dt.hour\n",
    "data['DAY_OF_WEEK'] = data['DATE OCC'].dt.dayofweek\n",
    "data['MONTH'] = data['DATE OCC'].dt.month\n",
    "\n",
    "# Seleccionar características y etiquetas\n",
    "X = data[['HOUR', 'DAY_OF_WEEK', 'MONTH']].values\n",
    "y = data['Crm Cd Desc'].astype(str)  # Convertir la columna a cadenas\n",
    "\n",
    "# Codificar las etiquetas\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Escalar características\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Ajustar las dimensiones para la entrada de LSTM\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Construir la red neuronal recurrente (LSTM)\n",
    "model = Sequential([\n",
    "    LSTM(64, input_shape=(X_train.shape[1], 1), activation='relu', return_sequences=True),\n",
    "    LSTM(32, activation='relu'),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Ajustar el modelo\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7d4af36c-4262-434c-8212-5543b09ef489",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_259/1900249212.py:12: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data['DATE OCC'] = pd.to_datetime(data['DATE OCC'], errors='coerce')  # Convertir fechas\n",
      "/opt/conda/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de muestras en X: 318977\n",
      "Número de etiquetas en y: 318977\n",
      "Epoch 1/20\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 487us/step - accuracy: 0.1009 - loss: 3.4538 - val_accuracy: 0.1146 - val_loss: 3.3759\n",
      "Epoch 2/20\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 478us/step - accuracy: 0.1067 - loss: 3.3831 - val_accuracy: 0.1146 - val_loss: 3.3689\n",
      "Epoch 3/20\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 474us/step - accuracy: 0.1070 - loss: 3.3793 - val_accuracy: 0.1146 - val_loss: 3.3705\n",
      "Epoch 4/20\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 475us/step - accuracy: 0.1079 - loss: 3.3744 - val_accuracy: 0.1146 - val_loss: 3.3679\n",
      "Epoch 5/20\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 473us/step - accuracy: 0.1068 - loss: 3.3721 - val_accuracy: 0.1160 - val_loss: 3.3666\n",
      "Epoch 6/20\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 476us/step - accuracy: 0.1094 - loss: 3.3710 - val_accuracy: 0.1146 - val_loss: 3.3748\n",
      "Epoch 7/20\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 473us/step - accuracy: 0.1076 - loss: 3.3711 - val_accuracy: 0.1146 - val_loss: 3.3687\n",
      "Epoch 8/20\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 474us/step - accuracy: 0.1083 - loss: 3.3690 - val_accuracy: 0.1146 - val_loss: 3.3709\n",
      "Epoch 9/20\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 475us/step - accuracy: 0.1078 - loss: 3.3692 - val_accuracy: 0.1145 - val_loss: 3.3666\n",
      "Epoch 10/20\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 472us/step - accuracy: 0.1092 - loss: 3.3672 - val_accuracy: 0.1146 - val_loss: 3.3687\n",
      "Epoch 11/20\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 479us/step - accuracy: 0.1092 - loss: 3.3677 - val_accuracy: 0.1146 - val_loss: 3.3674\n",
      "Epoch 12/20\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 480us/step - accuracy: 0.1080 - loss: 3.3696 - val_accuracy: 0.1146 - val_loss: 3.3700\n",
      "Epoch 13/20\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 481us/step - accuracy: 0.1093 - loss: 3.3709 - val_accuracy: 0.1159 - val_loss: 3.3650\n",
      "Epoch 14/20\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 479us/step - accuracy: 0.1095 - loss: 3.3658 - val_accuracy: 0.1160 - val_loss: 3.3684\n",
      "Epoch 15/20\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 480us/step - accuracy: 0.1099 - loss: 3.3653 - val_accuracy: 0.1160 - val_loss: 3.3701\n",
      "Epoch 16/20\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 481us/step - accuracy: 0.1084 - loss: 3.3649 - val_accuracy: 0.1160 - val_loss: 3.3719\n",
      "Epoch 17/20\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 481us/step - accuracy: 0.1108 - loss: 3.3639 - val_accuracy: 0.1160 - val_loss: 3.3686\n",
      "Epoch 18/20\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 480us/step - accuracy: 0.1082 - loss: 3.3654 - val_accuracy: 0.1160 - val_loss: 3.3791\n",
      "Epoch 19/20\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 481us/step - accuracy: 0.1088 - loss: 3.3655 - val_accuracy: 0.1146 - val_loss: 3.3712\n",
      "Epoch 20/20\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 480us/step - accuracy: 0.1093 - loss: 3.3640 - val_accuracy: 0.1146 - val_loss: 3.3724\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Cargar los datos\n",
    "data = pd.read_csv('LACrimesData.csv', low_memory=False)\n",
    "\n",
    "# Preprocesamiento de datos\n",
    "if 'DATE OCC' in data.columns:\n",
    "    data['DATE OCC'] = pd.to_datetime(data['DATE OCC'], errors='coerce')  # Convertir fechas\n",
    "    data = data.dropna(subset=['DATE OCC'])  # Eliminar filas con fechas no válidas\n",
    "    data = data.sort_values(by='DATE OCC')  # Ordenar cronológicamente\n",
    "\n",
    "    # Crear características temporales (día, mes, año)\n",
    "    data['DAY'] = data['DATE OCC'].dt.day\n",
    "    data['MONTH'] = data['DATE OCC'].dt.month\n",
    "    data['YEAR'] = data['DATE OCC'].dt.year\n",
    "\n",
    "    # Seleccionar características y etiquetas\n",
    "    X = data[['DAY', 'MONTH', 'YEAR']].values\n",
    "    y = data['Crm Cd Desc']  # Usar la descripción del crimen como etiqueta\n",
    "\n",
    "    # Verificar que X y y no están vacíos\n",
    "    print(\"Número de muestras en X:\", X.shape[0])\n",
    "    print(\"Número de etiquetas en y:\", len(y))\n",
    "\n",
    "    if X.shape[0] > 0 and len(y) > 0:\n",
    "        # Codificar las etiquetas\n",
    "        label_encoder = LabelEncoder()\n",
    "        y = label_encoder.fit_transform(y)\n",
    "\n",
    "        # Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "        # Escalar características\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # Ajustar las dimensiones para la entrada de CNN (reshape)\n",
    "        X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "        X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "        # Construir la red neuronal convolucional (CNN)\n",
    "        model = Sequential([\n",
    "    Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(50, activation='relu'),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "        # Compilar el modelo\n",
    "        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        # Ajustar el modelo\n",
    "        model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n",
    "    else:\n",
    "        print(\"Error: Conjunto de datos vacío después del preprocesamiento.\")\n",
    "else:\n",
    "    print(\"Error: Columna 'DATE OCC' no encontrada en los datos.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "343895e6-cdb6-43b1-b002-1afcd27d214f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_259/2366644716.py:8: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv('LACrimesData.csv')\n",
      "/tmp/ipykernel_259/2366644716.py:11: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data['DATE OCC'] = pd.to_datetime(data['DATE OCC'], errors='coerce')  # Convertir fechas\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 767us/step - accuracy: 0.1023 - loss: 3.4405 - val_accuracy: 0.1146 - val_loss: nan\n",
      "Epoch 2/20\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 760us/step - accuracy: 0.1050 - loss: 3.3845 - val_accuracy: 0.1146 - val_loss: nan\n",
      "Epoch 3/20\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 736us/step - accuracy: 0.1055 - loss: 3.3820 - val_accuracy: 0.1146 - val_loss: nan\n",
      "Epoch 4/20\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 738us/step - accuracy: 0.1048 - loss: 3.3860 - val_accuracy: 0.1146 - val_loss: nan\n",
      "Epoch 5/20\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 744us/step - accuracy: 0.1044 - loss: 3.3812 - val_accuracy: 0.1146 - val_loss: nan\n",
      "Epoch 6/20\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 758us/step - accuracy: 0.1054 - loss: 3.3813 - val_accuracy: 0.1146 - val_loss: nan\n",
      "Epoch 7/20\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 761us/step - accuracy: 0.1056 - loss: 3.3800 - val_accuracy: 0.1146 - val_loss: nan\n",
      "Epoch 8/20\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 763us/step - accuracy: 0.1049 - loss: 3.3781 - val_accuracy: 0.1146 - val_loss: nan\n",
      "Epoch 9/20\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 756us/step - accuracy: 0.1061 - loss: 3.3736 - val_accuracy: 0.1146 - val_loss: nan\n",
      "Epoch 10/20\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 756us/step - accuracy: 0.1054 - loss: 3.3786 - val_accuracy: 0.1146 - val_loss: nan\n",
      "Epoch 11/20\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 758us/step - accuracy: 0.1055 - loss: 3.3723 - val_accuracy: 0.1146 - val_loss: nan\n",
      "Epoch 12/20\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 759us/step - accuracy: 0.1062 - loss: 3.3765 - val_accuracy: 0.1146 - val_loss: nan\n",
      "Epoch 13/20\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 765us/step - accuracy: 0.1060 - loss: 3.3746 - val_accuracy: 0.1146 - val_loss: nan\n",
      "Epoch 14/20\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 758us/step - accuracy: 0.1057 - loss: 3.3735 - val_accuracy: 0.1146 - val_loss: nan\n",
      "Epoch 15/20\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 763us/step - accuracy: 0.1059 - loss: 3.3754 - val_accuracy: 0.1146 - val_loss: nan\n",
      "Epoch 16/20\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 757us/step - accuracy: 0.1070 - loss: 3.3732 - val_accuracy: 0.1146 - val_loss: nan\n",
      "Epoch 17/20\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 757us/step - accuracy: 0.1069 - loss: 3.3716 - val_accuracy: 0.1146 - val_loss: nan\n",
      "Epoch 18/20\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 747us/step - accuracy: 0.1048 - loss: 3.3720 - val_accuracy: 0.1146 - val_loss: nan\n",
      "Epoch 19/20\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 746us/step - accuracy: 0.1057 - loss: 3.3769 - val_accuracy: 0.1146 - val_loss: nan\n",
      "Epoch 20/20\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 746us/step - accuracy: 0.1048 - loss: 3.3717 - val_accuracy: 0.1146 - val_loss: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0xffff1bd32850>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Cargar los datos\n",
    "data = pd.read_csv('LACrimesData.csv')\n",
    "\n",
    "# Preprocesamiento de datos\n",
    "data['DATE OCC'] = pd.to_datetime(data['DATE OCC'], errors='coerce')  # Convertir fechas\n",
    "data = data.sort_values(by='DATE OCC')  # Ordenar cronológicamente\n",
    "\n",
    "# Crear características temporales (día, mes, año)\n",
    "data['HOUR'] = data['DATE OCC'].dt.hour\n",
    "data['DAY_OF_WEEK'] = data['DATE OCC'].dt.dayofweek\n",
    "data['MONTH'] = data['DATE OCC'].dt.month\n",
    "\n",
    "# Seleccionar características y etiquetas\n",
    "X = data[['HOUR', 'DAY_OF_WEEK', 'MONTH']].values\n",
    "y = data['Crm Cd Desc']  # Usar la descripción del crimen como etiqueta\n",
    "\n",
    "# Codificar las etiquetas\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Escalar características\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Ajustar las dimensiones para la entrada de ARNN\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Construir la red neuronal auto-recurrente (ARNN)\n",
    "model = Sequential([\n",
    "    SimpleRNN(64, input_shape=(X_train.shape[1], 1), activation='relu', return_sequences=True),\n",
    "    SimpleRNN(32, activation='relu'),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Ajustar el modelo\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b63b2de0-2bb3-4f50-bcfa-8ea6980e5e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_107/123312160.py:10: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv('LACrimesData.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 15ms/step - accuracy: 0.7763 - loss: 0.6256 - val_accuracy: 0.7793 - val_loss: 0.5844\n",
      "Epoch 2/10\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 15ms/step - accuracy: 0.7856 - loss: 0.5736 - val_accuracy: 0.7813 - val_loss: 0.5797\n",
      "Epoch 3/10\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 15ms/step - accuracy: 0.7908 - loss: 0.5588 - val_accuracy: 0.7818 - val_loss: 0.5799\n",
      "Epoch 4/10\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 15ms/step - accuracy: 0.7951 - loss: 0.5462 - val_accuracy: 0.7818 - val_loss: 0.5813\n",
      "Epoch 5/10\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 15ms/step - accuracy: 0.7972 - loss: 0.5401 - val_accuracy: 0.7816 - val_loss: 0.5847\n",
      "Epoch 6/10\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m483s\u001b[0m 61ms/step - accuracy: 0.8008 - loss: 0.5289 - val_accuracy: 0.7807 - val_loss: 0.5871\n",
      "Epoch 7/10\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 15ms/step - accuracy: 0.8079 - loss: 0.5144 - val_accuracy: 0.7808 - val_loss: 0.5968\n",
      "Epoch 8/10\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 15ms/step - accuracy: 0.8103 - loss: 0.5042 - val_accuracy: 0.7810 - val_loss: 0.6019\n",
      "Epoch 9/10\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 15ms/step - accuracy: 0.8134 - loss: 0.4936 - val_accuracy: 0.7779 - val_loss: 0.6090\n",
      "Epoch 10/10\n",
      "\u001b[1m7975/7975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 15ms/step - accuracy: 0.8184 - loss: 0.4796 - val_accuracy: 0.7774 - val_loss: 0.6211\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0xfffef0266010>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Cargar los datos\n",
    "data = pd.read_csv('LACrimesData.csv')\n",
    "\n",
    "# Preprocesamiento de datos\n",
    "X_title = data['Crm Cd Desc'].fillna('').values  # Utilizar \"Crm Cd Desc\" como el título\n",
    "X_location = data['LOCATION'].fillna('').values  # Utilizar \"LOCATION\" para la ubicación\n",
    "y = data['Status Desc'].fillna('').values  # Utilizar \"Status Desc\" como la descripción secundaria\n",
    "\n",
    "# Concatenar título y ubicación para el modelo de texto\n",
    "X = [f\"{title} {location}\" for title, location in zip(X_title, X_location)]\n",
    "\n",
    "# Codificar etiquetas\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Tokenización de texto\n",
    "tokenizer = Tokenizer(num_words=10000)  # Tamaño máximo del vocabulario\n",
    "tokenizer.fit_on_texts(X)\n",
    "X_tokenized = tokenizer.texts_to_sequences(X)\n",
    "X_padded = pad_sequences(X_tokenized, maxlen=50)  # Máximo de 50 palabras por entrada\n",
    "\n",
    "# División de datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Modelo de texto con RNN\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=64, input_length=50),\n",
    "    LSTM(64, return_sequences=True),\n",
    "    LSTM(32),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Ajustar el modelo\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0a7c4c-cc8f-4d87-8411-1d84f396191b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
